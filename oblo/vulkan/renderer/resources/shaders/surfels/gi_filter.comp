#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_control_flow_attributes : enable
#extension GL_EXT_buffer_reference : require
#extension GL_EXT_shader_8bit_storage : require
#extension GL_ARB_gpu_shader_int64 : require

#include <renderer/camera>
#include <renderer/instances>
#include <renderer/math>
#include <renderer/meshes/mesh_attributes>
#include <renderer/meshes/mesh_data>
#include <renderer/meshes/mesh_indices>
#include <renderer/meshes/mesh_table>
#include <renderer/transform>
#include <visibility/visibility_buffer>
#include <visibility/visibility_utils>

#include <postprocess/a_trous>

layout(binding = 0, rgba16f) uniform restrict readonly image2D t_InSource;
layout(binding = 1, rgba16f) uniform restrict writeonly image2D t_OutFiltered;
layout(binding = 3, rg32ui) uniform restrict readonly uimage2D t_InVisibilityBuffer;

layout(binding = 16) uniform b_CameraBuffer
{
    camera_buffer g_Camera;
};

shared vec3 g_IrradianceCache[g_ATrousCacheSize][g_ATrousCacheSize];
shared float g_DepthCache[g_ATrousCacheSize][g_ATrousCacheSize];
shared vec3 g_NormalCache[g_ATrousCacheSize][g_ATrousCacheSize];

void main()
{
    const uvec2 resolution = imageSize(t_InSource);

    a_trous_load_shared_cache(resolution);

    memoryBarrierShared();
    barrier();

    const uvec2 screenPos = gl_GlobalInvocationID.xy;

    if (screenPos.x >= resolution.x || screenPos.y >= resolution.y)
    {
        return;
    }

    const uvec2 center = uvec2(g_ATrousPadding) + gl_LocalInvocationID.xy;

    vec3 result = vec3(0);

    const float kernel[g_ATrousTapsPerPixel] = {1.f, 2.f / 3.f, 1.f / 6.f};

    float weightSum = 0.f;

    const vec3 irradianceCenter = g_IrradianceCache[center.y][center.x];
    const float depthCenter = g_DepthCache[center.y][center.x];
    const vec3 normalCenter = g_NormalCache[center.y][center.x];

    const int k = int(g_ATrousTapRadius);

    [[unroll]] for (int y = -k; y <= k; ++y)
    {
        [[unroll]] for (int x = -k; x <= k; ++x)
        {
            const uvec2 coords = uvec2(center + ivec2(x, y) * g_ATrousStride);
            const vec3 irradiance = g_IrradianceCache[coords.y][coords.x];
            const float depth = g_DepthCache[coords.y][coords.x];
            const vec3 normal = g_NormalCache[coords.y][coords.x];

            float w = kernel[abs(x)] * kernel[abs(y)];

            const float invDepthSigma = 1e2;
            w *= exp(-abs(depth - depthCenter) * invDepthSigma);
            w *= pow(saturate(dot(normal, normalCenter)), 32.f);

            result += w * irradiance;

            weightSum += w;
        }
    }

    if (weightSum > 1e-6)
    {
        result /= weightSum;
    }

    imageStore(t_OutFiltered, ivec2(screenPos), vec4(result, 1));
}

void sample_geometry(
    in visibility_buffer_data vb, in ivec2 coords, in uvec2 resolution, out float depth, out vec3 normalWS)
{
    const mesh_handle mesh = OBLO_INSTANCE_DATA(vb.instanceTableId, i_MeshHandles, vb.instanceId);
    const transform transform = OBLO_INSTANCE_DATA(vb.instanceTableId, i_TransformBuffer, vb.instanceId);

    // Read the mesh data
    const mesh_table meshTable = mesh_table_fetch(mesh);

    const uvec3 vertexIndices = mesh_get_meshlet_indices(meshTable, mesh, vb.meshletId, vb.meshletTriangleId);

    triangle triangleWS;
    vec3 triangleNormal[3];

    // Read the vertex data and transform everything in world space, we do lighting calculation in that
    // space
    [[unroll]] for (uint i = 0; i < 3; ++i)
    {
        const uint vertexId = vertexIndices[i];

        const vec3 vertexPosition = mesh_get_position(meshTable, vertexId);
        triangleWS.v[i] = (transform.localToWorld * vec4(vertexPosition, 1)).xyz;

        const vec3 vertexNormal = mesh_get_normal(meshTable, vertexId);
        triangleNormal[i] = vertexNormal;
    }

    const vec2 ndc = screen_to_ndc(uvec2(coords), resolution);

    const ray cameraRay = visibility_calculate_camera_ray(g_Camera, ndc);

    float intersectionDistance;

    distance_from_triangle_plane(cameraRay, triangleWS, intersectionDistance);

    const vec3 positionWS = ray_point_at(cameraRay, intersectionDistance);

    barycentric_coords bc;
    barycentric_calculate(bc, triangleWS.v, positionWS);

    depth = intersectionDistance;

    // Note that we are ignoring normal mapping here, which is probably an ok compromise
    const vec3 N = barycentric_interpolate(bc, triangleNormal);
    normalWS = normalize(mat3(transform.normalMatrix) * N);
}

void a_trous_load_pixel(in ivec2 coords, in uvec2 offset, in uvec2 resolution)
{
    const vec4 pixel = imageLoad(t_InSource, coords);

    g_IrradianceCache[offset.y][offset.x] = pixel.xyz;

    const uvec4 vbPixel = imageLoad(t_InVisibilityBuffer, coords);

    visibility_buffer_data vb;

    const bool isGeometrySample = visibility_buffer_parse(vbPixel.xy, vb);

    float depth;
    vec3 normalWS;

    if (visibility_buffer_parse(vbPixel.xy, vb))
    {
        sample_geometry(vb, coords, resolution, depth, normalWS);
    }
    else
    {
        // We use negative depth to signify no geometry (e.g. sky)
        depth = -1.f;
    }

    g_DepthCache[offset.y][offset.x] = depth;
    g_NormalCache[offset.y][offset.x] = normalWS;
}