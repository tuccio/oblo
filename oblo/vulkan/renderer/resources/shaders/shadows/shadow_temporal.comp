#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_control_flow_attributes : enable
#extension GL_EXT_buffer_reference : require
#extension GL_EXT_shader_8bit_storage : require
#extension GL_ARB_gpu_shader_int64 : require

#include <renderer/camera>
#include <renderer/instances>
#include <renderer/math>
#include <renderer/meshes/mesh_attributes>
#include <renderer/meshes/mesh_data>
#include <renderer/meshes/mesh_indices>
#include <renderer/meshes/mesh_table>
#include <renderer/transform>
#include <visibility/visibility_buffer>
#include <visibility/visibility_utils>

layout(binding = 0, r8) uniform restrict readonly image2D t_InShadow;
layout(binding = 1, r8) uniform restrict readonly image2D t_InShadowMean;
layout(binding = 2, r8) uniform restrict readonly image2D t_InHistory;
layout(binding = 3, rg16f) uniform restrict writeonly image2D t_OutShadowMoments;

layout(binding = 5, r8ui) uniform restrict uimage2D t_InOutHistorySamplesCount;

layout(binding = 8, r8) uniform restrict writeonly image2D t_OutFiltered;
layout(binding = 9, rg32ui) uniform restrict readonly uimage2D t_InVisibilityBuffer;

layout(binding = 16) uniform b_CameraBuffer
{
    camera_buffer g_Camera;
};

// We run 8x8 groups
const uint g_ThreadLocalSize = 8;

const uint g_ShadowCacheSize = g_ThreadLocalSize;
const uint g_ShadowLoadsPerThread = (g_ShadowCacheSize + g_ThreadLocalSize - 1) / g_ThreadLocalSize;

layout(local_size_x = g_ThreadLocalSize, local_size_y = g_ThreadLocalSize, local_size_z = 1) in;

ivec2 apply_offset_clamp(in ivec2 pixel, in uvec2 offset, in uvec2 resolution)
{
    return ivec2(min(int(resolution.x), int(pixel.x + offset.x)), min(int(resolution.y), int(pixel.y + offset.y)));
}

ivec2 reproject_coords(in visibility_buffer_data vb, in ivec2 coords, in uvec2 resolution)
{
    const mesh_handle mesh = OBLO_INSTANCE_DATA(vb.instanceTableId, i_MeshHandles, vb.instanceId);
    const transform transform = OBLO_INSTANCE_DATA(vb.instanceTableId, i_TransformBuffer, vb.instanceId);

    // Read the mesh data
    const mesh_table meshTable = mesh_table_fetch(mesh);

    const uvec3 vertexIndices = mesh_get_meshlet_indices(meshTable, mesh, vb.meshletId, vb.meshletTriangleId);

    triangle triangleWS;
    triangle prevTriangleWS;

    // Read the vertex data and transform everything in world space, we do lighting calculation in that
    // space
    [[unroll]] for (uint i = 0; i < 3; ++i)
    {
        const uint vertexId = vertexIndices[i];

        const vec3 vertexPosition = mesh_get_position(meshTable, vertexId);
        triangleWS.v[i] = (transform.localToWorld * vec4(vertexPosition, 1)).xyz;

        prevTriangleWS.v[i] = (transform.lastFrameLocalToWorld * vec4(vertexPosition, 1)).xyz;
    }

    const vec2 ndc = screen_to_ndc(uvec2(coords), resolution);

    const ray cameraRay = visibility_calculate_camera_ray(g_Camera, ndc);

    // We could use this for the depth stop function for the à trous filter
    float intersectionDistance;

    distance_from_triangle_plane(cameraRay, triangleWS, intersectionDistance);

    const vec3 positionWS = ray_point_at(cameraRay, intersectionDistance);

    barycentric_coords bc;
    barycentric_calculate(bc, triangleWS.v, positionWS);

    const vec2 prevNDC =
        visibility_calculate_last_frame_position_ndc_2d(ndc, g_Camera.lastFrameViewProjection, bc, prevTriangleWS);

    const ivec2 pixelOffset = ivec2(resolution * (prevNDC - ndc));

    return pixelOffset + coords;
}

void main()
{
    const uvec2 resolution = imageSize(t_InShadow);
    const uvec2 screenPos = gl_GlobalInvocationID.xy;

    if (screenPos.x >= resolution.x || screenPos.y >= resolution.y)
    {
        return;
    }

    const uvec4 vbPixel = imageLoad(t_InVisibilityBuffer, ivec2(screenPos));

    visibility_buffer_data vb;

    if (!visibility_buffer_parse(vbPixel.xy, vb))
    {
        // TODO: If nothing is in the VB of a whole tile we could probably skip the whole filtering as well
        imageStore(t_OutFiltered, ivec2(screenPos), vec4(0, 0, 0, 1));
        imageStore(t_InOutHistorySamplesCount, ivec2(screenPos), uvec4(0));
        return;
    }

    const float shadow = imageLoad(t_InShadow, ivec2(screenPos)).x;
    const float mean = imageLoad(t_InShadowMean, ivec2(screenPos)).x;

    const ivec2 historyCoords = reproject_coords(vb, ivec2(screenPos), resolution);

    float history = 0.f;
    uint historySamplesCount = 0;
    float historyWeight = 0.f;

    if (historyCoords.x >= 0 && historyCoords.x < resolution.x && historyCoords.y >= 0 &&
        historyCoords.y < resolution.y)
    {
        history = imageLoad(t_InHistory, historyCoords).x;
        historySamplesCount = imageLoad(t_InOutHistorySamplesCount, historyCoords).x;
        historyWeight = .3f;
    }

    // TODO: If the mean is 0 or 1, we might also skip the whole filtering
    // TODO: We should implement edge-stopping (see EAW à trous)
    // TODO: Should probably check for disocclusion

    const float mean2 = mean * mean;
    const float variance = saturate(mean - mean2);

    if (variance > .01f)
    {
        historyWeight *= 3.f;
    }

    const float deviation = sqrt(variance);
    const float hDeviation = .5f * deviation;

    const float nmin = mean - hDeviation;
    const float nmax = mean + hDeviation;

    const float clampedHistory = clamp(history, nmin, nmax);

    const float result = mix(shadow, clampedHistory, historyWeight);

    imageStore(t_OutFiltered, ivec2(screenPos), vec4(result, 0, 0, 1));

    const bool acceptSample = true;
    const uint newSamples = max(acceptSample ? historySamplesCount + 1 : 1, 255);
    imageStore(t_InOutHistorySamplesCount, ivec2(screenPos), uvec4(newSamples, 0, 0, 0));

    imageStore(t_OutShadowMoments, ivec2(screenPos), vec4(mean, mean2, 0, 0));
}